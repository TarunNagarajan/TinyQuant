# Task-Aware Selective Quantization for LLMs

Standard post-training quantization methods often degrade model performance on reasoning-heavy tasks by applying uniform compression policies. This project investigates *Task-Aware Selective Quantization* on Microsoft Phi-2 2.7B and Llama-3 3B, leveraging Fisher Information metrics to identify and preserve the "structural load-bearing" components of the models.

## Model Dynamics & Sensitivity Physics

The Fisher Information maps generated in this study reveal distinct "physics" for different architectures. These sensitivity scores are not random; they correlate directly with the functional roles of specific layers and the cognitive demands of the GSM8K (Grade School Math) dataset. This analysis supports the implementation of a "Task-Aware" strategy that goes beyond simple uniform quantization.

![Phi-2 Quantization Regions](fisher/reports/phi/quantization_regions_fisher_gsm8k_mean.png)

The sensitivity distribution in **Microsoft Phi-2** precisely mirrors the cognitive load of solving math word problems via Chain-of-Thought (CoT). We observe an unusually high sensitivity in the very first **Value Projection (Layer 0 `v_proj`)**, which acts as the model's "Working Memory." GSM8K problems typically begin with variable initialization, and this layer effectively "binds" these raw token integers into the semantic residual stream. This "Copy-Paste" mechanism implies that if Layer 0 is quantized, the variables are corrupted at the source, meaning the downstream reasoning engine operates perfectly on incorrect numbers. The prompt structure—concise Q&A pairs—imposes a rigid "rhythm" on generation, further amplifying the importance of this layer for "mode switching" between passive reading and active reasoning.

In the middle layers (6-20), the sensitivity shifts toward the **MLP blocks**, representing the "Reasoning Engine." Here, the MLPs function as key-value memories that perform the actual arithmetic transformations. Their sustained sensitivity—higher than Attention in these layers—confirms that computation is the bottleneck here, not context retrieval. However, the use of concise 4-shot CoT prompts concentrates attention on numerical operations and strips away linguistic "fluff," rendering these middle layers surprisingly robust to INT4 quantization. They can handle aggressive compression provided the input signal remains pristine.

The most critical phase transition for Phi-2 occurs at **Layer 29**, specifically in the `input_layernorm` and `self_attn.k_proj`. This layer acts as a "Coherence Aggregator" or "Event Horizon" for the Chain-of-Thought process. At this depth, the model is not performing new arithmetic but attending back to the entire generated trace to synthesize the final conclusion. The extreme sensitivity here proves the model is actively using the few-shot examples as dynamic templates for reasoning structure. Quantizing this layer severs the link between the prompt's instructions and the generation, destroying the model's ability to "trust" its own logic chain and leading to coherent but factually incorrect hallucinations.

![Llama-3 3B Quantization Regions](fisher/reports/llama_3b/quantization_regions_fisher_gsm8k_mean.png)

**Llama-3 3B** exhibits a fundamentally different mechanical dynamic, characterized by a massive anomaly in **Layer 1 `down_proj`**. This single layer displays sensitivity scores orders of magnitude higher than the rest of the network. This is an artifact of the **SwiGLU activation function** and the specific architecture of Llama, where the up-projection generates a vast amount of high-dimensional features. Layer 1 `down_proj` functions as a "High-Pass Filter" or "Cleanup Artifact," aggressively suppressing incoherent dimensions and noise introduced by the initial embedding expansion. It is not performing reasoning; it is performing signal cleaning. Quantizing this layer prevents noise suppression, flooding the residual stream with high-frequency garbage that destabilizes all subsequent attention heads. Similar to Phi-2, Llama-3 3B shows high sensitivity in its anchor points—**Input Embeddings** and **Language Modeling Head**. These layers define the semantic subspace, and quantizing them distorts the entire coordinate system of the model.

Comparing these two models illustrates that "General Purpose Quantization" is flawed. **Phi-2** operates as a "Bottom-Heavy, Top-Heavy" system: it demands extreme precision at the start to load variables and at the end to synthesize the answer, while its middle "Reasoning Engine" is computationally robust. It relies on preserving **Signal Integrity** through the LayerNorms and specific Attention projections to keep the linear logic path straight, a requirement driven by the sequential dependency of the GSM8K arithmetic tasks. **Llama-3 3B**, conversely, is a "Filter-Dependent" machine. Its stability relies entirely on specific high-precision gates (like the Layer 1 down-projection) to manage its own internal noise floor. While Phi-2 fails if its logic chain is disrupted (Coherence failure), Llama-3 3B fails if its signal-to-noise ratio drops below a critical threshold (Filtering failure).

![Phi-2 Task Sensitivity: GSM8K vs MATH](fisher/maps/phi/phi_fisher_gsm8k_vs_math.png)

Furthermore, comparisons between GSM8K (Arithmetic) and MATH (Algebra) maps on Phi-2 reveal that while GSM8K stresses the structural components (LayerNorms), harder Algebra problems require higher fidelity in the deep MLPs to handle complex variable transformations. This confirms that for arithmetic tasks, we must prioritize the "Signal Integrity" of the attention path, while for complex algebra, we must preserve the "Compute Capacity" of the MLPs.

### Magnitude vs. Fisher: The "Silent" Layer Trap

Comparing Magnitude-based sensitivity maps ($|W|$) with Fisher Information maps ($\partial \mathcal{L}/\partial W$) reveals a dangerous divergence in how importance is assigned, particularly for reasoning tasks.

![Fisher vs Magnitude Comparison (Phi-2)](fisher/maps/phi/gsm8k_mean_fisher_vs_magnitude.png)

For **Phi-2**, the divergence is critical at the "Coherence Gate" (Layer 29). The Magnitude map assigns the `k_proj` weights in this layer a very low score, one of the lowest in the entire model. A magnitude-based policy would immediately quantize this layer, assuming it is insignificant. However, the Fisher map identifies it as a top-3 sensitivity hotspot. This indicates that while these weights are numerically small, they are performing delicate, high-precision work—specifically, the retrieval of reasoning templates from the context window. Magnitude metrics fail to capture this "Information Density," mistaking small weights for unimportant ones. 

![Fisher vs Magnitude Comparison (Llama-3 3B)](fisher/maps/llama_3b/gsm8k_mean_fisher_vs_magnitude.png)

For **Llama-3 3B**, the divergence appears in the "Cleanup" mechanism. The Magnitude map emphasizes the massive weights in the deep attention heads (Layers 29-31 `q_proj`), assigning them top priority, but completely misses the anomaly in **Layer 1**, assigning it average importance. Fisher Information, however, flags Layer 1 as the single most critical component for stability. This suggests that Magnitude prioritizes "Structural Mass" (layers that store knowledge), whereas Fisher prioritizes "Information Velocity" (layers that transform or filter the signal flow). Relying on Magnitude for Llama-3 would preserve the deep "knowledge" layers while destroying the early "noise filter," rendering the preserved knowledge inaccessible due to upstream signal corruption.

### Inference: Architectural Physics & Information Density

The divergence in sensitivity profiles allows us to infer the fundamental "Computational Density" of these architectures. **Phi-2** functions as a highly specialized, **low-entropy system**. Its sensitivity map is "sparse," characterized by sharp peaks (Layers 0, 29) and deep valleys (Layers 6-20). This implies a rigid functional specialization: the model has compartmentalized its logic into discrete stages—Assembly, Computation, and Synthesis. It processes tasks linearly, mimicking the temporal structure of the Chain-of-Thought. While this makes the middle layers highly compressible (low information density), it renders the model structurally brittle; a single point-failure in a high-density layer (like Layer 29) collapses the entire reasoning pipeline.

**Llama-3 3B**, in contrast, operates as a **high-entropy, high-density system**. Aside from the massive filtration spike in Layer 1, its baseline sensitivity is more uniformly distributed and generally higher than Phi-2’s middle layers. This suggests a "holographic" processing style where information is not strictly compartmentalized but is redundantly encoded across the depth of the network. Llama-3 relies on generating a massive surplus of high-dimensional signals (via SwiGLU) and then aggressively filtering them. This redundancy makes it more robust to "structural" quantization errors (drift in one layer is corrected by the next) but paradoxically makes it highly sensitive to the "noise floor." It processes tasks not by simulating a linear pipeline, but by iteratively refining a noisy signal manifold. Consequently, Llama-3 requires a quantization strategy that prioritizes *Global Signal Quality* (filtering), whereas Phi-2 requires one that prioritizes *Functional Component Preservation* (specific gates).