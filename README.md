# Task-Aware Selective Quantization for LLMs
Standard post-training quantization methods often degrade model performance on reasoning-heavy tasks by applying uniform compression policies. This project investigates *Task-Aware Selective Quantization* on Qwen2.5-Math-1.5B.

Domain sensitivity analysis performed on: 
- Target Domain: GSM8K
- Control Domain: WikiText-2

