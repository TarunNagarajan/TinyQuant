{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: Computing Sensitivity Layer-by-Layer\n",
        "\n",
        "This experiment computes layer sensitivity using two methods: the Fisher Information Matrix and the L1 Norm (Magnitude). Magnitude pruning acts as a blunt instrument and fails to distinguish layer importance, whereas Fisher Information reveals a high dynamic range with sharp peaks of sensitivity across the network.\n",
        "\n",
        "---\n",
        "\n",
        "## Magnitude vs Fisher Behavior\n",
        "\n",
        "In `magnitude_gsm8k.json`, the sensitivity scores for MLP layers cluster tightly. For example:\n",
        "\n",
        "- Layer 4: 440,320  \n",
        "- Layer 5: 430,080  \n",
        "- Layer 6: 417,792  \n",
        "\n",
        "These values are nearly identical, demonstrating that magnitude-based scoring does not meaningfully separate critical vs. non-critical components.\n",
        "\n",
        "In contrast, `fisher_gsm8k.json` exhibits large variations in importance. For instance:\n",
        "\n",
        "- Layer 1 (down_proj): 8.47  \n",
        "- Layer 0 (gate_proj): 0.33  \n",
        "\n",
        "This represents a ~25Ã— difference in sensitivity.\n",
        "\n",
        "---\n",
        "\n",
        "## Peak Sensitivity Values\n",
        "\n",
        "The highest non-embedding Fisher values are:\n",
        "\n",
        "| Dataset     | Highest Layer | Score |\n",
        "|-------------|---------------|-------|\n",
        "| GSM8K (Math) | layers.0.self_attn.v_proj | 22.0 |\n",
        "| WikiText (English) | layers.0.self_attn.v_proj | 431.0 |\n",
        "\n",
        "In both datasets, `model.layers.0.self_attn.v_proj` is the single most sensitive weight matrix.\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset-Dependent Interpretation\n",
        "\n",
        "Early MLP layers are disproportionately important in mathematical reasoning relative to general language modeling.\n",
        "\n",
        "Example: Layer 1 MLP `down_proj`\n",
        "\n",
        "- Math context: score = **8.47**  \n",
        "  - Relative to max score (22.0): ~**38%**\n",
        "- English context: score = **18.57**  \n",
        "  - Relative to max score (431.0): ~**4%**\n",
        "\n",
        "So sensitivity structure is task-dependent, and mathematical reasoning utilizes early MLP pathways more aggressively than general English workloads.\n",
        "\n",
        "---\n",
        "\n",
        "## Attention Projection Sensitivity\n",
        "\n",
        "Across both datasets, Value projections (`v_proj`) dominate sensitivity, while Query projections (`q_proj`) are nearly irrelevant for quantization stability.\n",
        "\n",
        "Example from `fisher_gsm8k.json`:\n",
        "\n",
        "- Layer 0 v_proj: **21.97** (critical)\n",
        "- Layer 0 q_proj: **0.04** (negligible)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5ZWevbHVWrB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "stderr = sys.stderr\n",
        "sys.stderr = open(os.devnull, 'w')"
      ],
      "metadata": {
        "id": "K0OfVjcnRPTO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWeBUu2Tqqsc"
      },
      "outputs": [],
      "source": [
        "!pip install torch transformers accelerate datasets bitsandbytes scipy tqdm numpy matplotlib sentencepiece protobuf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/TarunNagarajan/TaskQuant.git"
      ],
      "metadata": {
        "id": "W9uATk_l7ML7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "id": "PsVQ4DIrKYw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import logging\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.getLogger('datasets.load').setLevel(logging.CRITICAL)"
      ],
      "metadata": {
        "id": "9r5MZUl3J9w7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
      ],
      "metadata": {
        "id": "wVFT2VNtQ8P0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "wCAzluS4RIYU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd TaskQuant\n",
        "!git pull"
      ],
      "metadata": {
        "id": "0pOq6PUv7R_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fisher Computation: GSM8K\n"
      ],
      "metadata": {
        "id": "KjhtpzfX8z0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/01_compute_sensitivity.py --method fisher --dataset gsm8k"
      ],
      "metadata": {
        "id": "zUctFok37UWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Magnitude: GSM8K"
      ],
      "metadata": {
        "id": "S8einBQVDeym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/01_compute_sensitivity.py --method magnitude --dataset gsm8k"
      ],
      "metadata": {
        "id": "DfyvVE9R-LZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fisher Computation: Wikitext 2"
      ],
      "metadata": {
        "id": "W0-_WZwIDqqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/01_compute_sensitivity.py --method fisher --dataset wikitext"
      ],
      "metadata": {
        "id": "08osu6sv-U77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Magnitude: Wikitext 2"
      ],
      "metadata": {
        "id": "IG0LEqBODz6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python scripts/01_compute_sensitivity.py --method magnitude --dataset wikitext"
      ],
      "metadata": {
        "id": "Qd10XVRt-Z0z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}